% !TEX root = thesis.tex
\documentclass[thesis]{subfiles}

\begin{document}
% ************************** Thesis Abstract *****************************
% Use `abstract' as an option in the document class to print only the titlepage and the abstract.
\begin{abstract}
	Deep learning has in recent years come to dominate the previously separate fields of research in machine learning, computer vision, natural language understanding and speech recognition. Despite breakthroughs in training deep networks, many relatively simple questions remain unanswered about the representations learned within these networks. In addition this lack of understanding in both the optimization and structure of deep networks has meant that contemporary deep network architectures for image classification have high computational and memory complexity. This is a direct result of the inability to identify the optimal architecture for datasets. 
	
	The approach advocated by many researchers in the field has been to train monolithic networks with excess complexity, and strong regularization -- an approach that has, in practice, found success in improving the generalization of deep networks, but leaves much to desire in efficiency. Instead, within this work, we propose that carefully designing networks in consideration of our prior knowledge of the task can improve the memory and compute efficiency of state-of-the art networks, and even increase accuracy through structurally induced regularization -- what we propose to denote \emph{structural priors}.

	We present two such structural priors for deep convolutional neural networks (CNNs), and evaluate them in state-of-the-art image classification architectures. Both methods are found to improve the generalization of these architectures while also decreasing the size and increasing the efficiency of their training and test time computation. 

	Finally, we begin to explore automatically finding structural priors in deep networks by presenting our work on adding conditional computation to deep networks. Although results show this is not optimal, the method shows promise in producing a better compute-generalization trade-off than naive approaches to reducing computation in deep networks, hopefully leading towards a better approach to automatically learning structural priors in deep networks.
\end{abstract}

\end{document}
