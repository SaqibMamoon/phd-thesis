\documentclass[thesis]{subfiles}

\begin{document}
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

\begin{chapquote}{Marvin Minsky, \textit{Prologue: A View from 1988, Perceptrons}}
``The marvelous powers of the brain emerge not from any single, uniformly structured connectionist network but from highly evolved arrangements of smaller, specialized networks which are interconnected in very specific ways.''
\end{chapquote}

%********************************** %First Section  **************************************
Deep learning has in recent years come to dominate the previously separate fields of research in machine learning, computer vision, natural language understanding and speech recognition. That these fields would have been considered relatively distinct less than 5 years ago belies the power of the method. Deep learning is of course only the latest in a long history of connectionist learning research, and while the breakthroughs in training deep networks are real, and the research community has continued to discover increasingly more applications for deep learning, many relatively simple questions of learning in neural networks remain unanswered.

Although it has been proven that a neural network with an infinitely wide single hidden layer is a universal approximator~\citep{journals/mcss/Cybenko92,hornik89a},  theoretically able to learn any function, in practice the architecture of a neural network has an unreasonably large affect on the generalization of a trained network. For example, it has been well demonstrated that for image classification, a reasonably designed convolutional neural network (CNN) as proposed by~\citet{Lecun1998} will always outperform a fully-connected network, no matter the number of hidden units learned. This is despite the fact that, as explained in \cref{background}, any convolutional neural network can be represented exactly in a (larger) fully connected network where shared parameters are replaced by duplicated weights and filter connectivity is simply represented by zeroed out weights.

This lack of understanding in both the optimization and structure of deep networks has meant that contemporary deep network architectures for image classification have high computational and memory complexity. This is a direct result of the inability to identify the optimal architecture for datasets. Instead, the approach advocated by many researchers in the field has been to train monolithic networks with excess complexity, and strong regularization -- an approach that has found success in practice for accuracy, but leaves much to desire in efficiency.

%While many have attributed the deep learning revolution to the availability of large datasets, such as Imagenet~\cite{ILSVRC2015}, and more powerful hardware, algorithmic improvements have also had a role to play. First and foremost, for any image related task, Convolutional Neural Networks

Instead, in this work, we propose that carefully designing networks in consideration of our prior knowledge of the task can improve the memory and compute efficiency of state-of-the art networks, and even increase accuracy through structurally induced regularization. This is not a new idea -- Convolutional Neural Networks are the most successful example of this approach, sparsifying neural networks to exploit our prior knowledge of the importance of local correlations in natural images.

While this philosophy defines our approach, deep neural networks have a large number of degrees of freedom, and there are many facets of deep neural networks that warrant such analysis. So as to not conflate the methods and results, we have attempted to present each of these in isolation. This thesis is organised as follows:
\begin{description}
	\item[\Cref{background}] explores the background of decision forests, neural networks and their applications, with particular emphasis on image classification.
	
	\item[\Cref{lowrankfilters}] addresses the spatial extents of convolutional filters, proposing to exploit our knowledge of the low-rank nature of most filters learned for image recognition by structuring a deep network to learn a low-rank basis for filters.
	
	Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. 
	
	We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs requiring much less compute. %Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41\% less compute and only 24\% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point {\em increase} in accuracy over our improved VGG-11 model, giving a top-5 \emph{center-crop} validation accuracy of 89.7\% while reducing computation by 16\% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26\% less compute and 41\% fewer model parameters. Applying our method to a near state-of-the-art network for CIFAR, we achieved comparable accuracy with 46\% less compute and 55\% fewer parameters. 
	
	\item[\Cref{deeproots}] addresses the filter/channel extents of convolutional filters, by learning filters with limited channel extents. A new method for creating computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. This allows a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy. 
	
	We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less compute. %, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40\% fewer parameters, 45\% fewer floating point operations, and is 31\% (12\%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25\% fewer floating point operations and 44\% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7\% fewer parameters and is 21\% (16\%) faster on a CPU (GPU).
	
	\item[\Cref{conditionalnetworks}] presents work towards conditional computation in deep neural networks, conditional networks, allowing for faster inference, by understanding the connections between two state of the art classifiers: deep neural networks (DNNs) and randomized decision forests.
	
	We propose a new discriminative learning model, \emph{conditional networks}, 
	that jointly exploits the accurate \emph{representation learning} capabilities of deep neural networks with the efficient \emph{conditional computation} of decision trees and directed acyclic graphs (DAGs).
	Conditional networks can be thought of as a way to learn an optimal block-diagonal sparsification of a DNN, and we show how they can be trained to cover the continuous spectrum between deep networks and decision forests/jungles. 
	
	In addition to improving test and training efficiency, conditional networks yield smaller models, are highly interpretable, and offer test-time flexibility. Validation is performed on standard image classification tasks. Compared to the state of the art, our results demonstrate superior efficiency for at-par accuracy both on the ImageNet and CIFAR datasets.
	
	\item[\Cref{futurework}] looks at the research questions that arise from the work presented in this thesis, in particular the hidden design in ``end-to-end'' learning, the effectiveness of structural priors, the and ineffectiveness of current optimization methods. Proposals are made for future research directions which seem pertinent given our results.
	
\end{description}

\end{document}
