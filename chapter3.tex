\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Methodology}
\label{methodology}
% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Figs/Raster/}{Figs/PDF/}{Figs/}}
\else
    \graphicspath{{Figs/Vector/}{Figs/}}
\fi

\section{Generalising Neural Networks and Decision Trees}
Although the relationship between neural networks and decision trees has been explored in the past, in the case of Entropy Nets~\cite{Sethi1990} this was with the intention of training and creating neural networks with decision-tree approaches, while in the case of TODO~\cite{Welbl2014casting}.

Here we explore the relationship, with the objective of reducing the connectivity of deep neural networks trained with back-propagation, specifically convolutional neural networks. 

Towards this objective, we generalize neural networks and decision trees intuitively by using a new graphical notation for representing both. This notation isolates the differences between the two models, such that we can represent a hybrid model, \ie a Conditional Network, compactly\footnote{This notation itself was created by Dr. Antonio Criminisi, and is not a contribution of this thesis. Some figures used with the permission of Dr. Antonio Criminisi/Microsoft Research.}.

\section{A New Graphical Notation}
The proposals we will make require a re-interpretation of existing classification models, that is neural networks and decision trees, but standard notation for both of these models hides the implicit functional similarities on which we will build our models. As such, before we are able to explain the concept of a conditional network, a new graphical notation is proposed.

\begin{figure}[htbp!] 
\centering
\includegraphics[width=1.0\textwidth]{newGraphLanguage}
\caption[New graphical notation for a standard neural network with one hidden layer.]{{\bf The proposed compact graphical notation for neural networks}. Non-linear transformations in a standard neural network with one hidden layer are indicated by the projection matrix $\mat P$ between the two layers, followed by a generic non-linearity, represented with the symbol $\wr$. \copyright Antonio Criminisi}
\label{fig:newGraphLanguage}
\end{figure}

The standard depiction of a neural network with one hidden layer is shown in Fig.~\ref{fig:newGraphLanguage}a., where each of the layers is fully connected, and these connecting weights are illustrated as lines between the neurons represented as circles. While this image illustrates the connectivity of the model well, it assumes the function of the neurons themselves to be known or otherwise described. In Fig.~\ref{fig:newGraphLanguage}b.\ we use a different notation to show both connectivity and function of each layer, with the assumption that all nodes on a particular layer have the same function.

In this simple example of a fully-connected neural network, between layers $i$ and $j$, every node outputs the non-linear transformation, $\vec v_j = \sigma(\mat P \vec v_i)$, a composition of the non-linear function $\sigma$ (\eg a ReLU or sigmoid) and the projection of the input units with a projection matrix $\mat P$, which includes the bias term in homogeneous coordinates. We denote this operation explicitly as $\mat P_{ij} \wr$, where $\mat P_{ij}$ is the projection matrix and $\wr$ represents a non-linearity. In short $\mat P_{ij} \wr$ denotes the standard neural net layer's non-linear transformation $\vec v_j = \sigma(\mat P \vec v_i)$. 

\begin{figure}[htbp!] 
\centering
\includegraphics[width=0.5\textwidth]{decisionTree.png}
\caption[New graphical notation for a binary decision tree.]{{\bf The proposed compact graphical notation for a decision tree}. \copyright Antonio Criminisi}
\label{fig:decisionTree}
\end{figure}

This notation equally applies to decision trees, as illustrated in Fig.~\ref{fig:decisionTree}.

\end{document}