\documentclass[thesis]{subfiles}

\begin{document}
	\chapter{The Unreasonable Affect of Structure on Learning in Neural Networks}
	\label{motivation}
	
	It is well known that the design of a neural network architecture can have a large effect on the generalization of a learned model. And yet, despite this, network design itself remains poorly understood, and is considered somewhat of a black magic, with intuition and experience being the cited motivation behind most common architectures, rather than any theory. This, more than perhaps any other factor, has been a barrier to access for the field.
	
	Beyond hyper-parameters used for tuning the optimization method, such as learning rate, momentum and weight decay, the architecture of a network has a profound effect on the learning. Nowhere is this effect more pronounced than in the case of using neural networks with highly structured inputs, such as natural images. Although neural networks are usually posed as general learning machines, time and again it has been demonstrated that neural networks only truly stand out as a learning method when we encode our prior knowledge of the task in the architecture itself -- what we will denote \emph{structural priors}. 
	
	Neural Networks with structural priors still differ significantly from hand-tuned local features, as popularized in computer vision in the early 2000s, such as SIFT~\citep{Lowe2004}. As compared with neural networks, such local features are rigidly defined in terms of structure and weights, and the learning system is restricted to finding and cataloging the pre-determined features in images. Neural networks with structural priors on the other hand, while restricting the structure of the network somewhat, still allow the network to learn more fine-grained structure, and have no effect on the latitude given to learning weights.
	
	% This is why NN are not good at general learning, and we are now only good at expert systems
	
	The history of understanding the role of neural network architecture in learning is long, arguably going back to the Hebbian rule of learning~\citep{hebb1949organization}, and yet our understanding is still far from complete. In this section we will review a select number of the most important previous works relevant to understanding the role that structural priors play.
	
    \section{Model Capacity and Representational Power}
    The information theoretic notion of capacity, that is the expressive power of a classification algorithm, gives important insights to the learning ability of a classification. Analysis is typically based on the Vapnikâ€“Chervonenkis (VC) dimension of the algorithm~\citep{vapnik2015uniform}. Intuitively, the VC dimension measures the complexity of the model. For example, back in the space of polynomial fits, a high degree polynomial has more capacity to learn complex functions, and thus can fit a training set well. This does not mean it will generalize well, and indeed its large capacity to learn the training set may result in overfitting.
    
    In the case of neural networks, early work showed that the capacity of neural networks is quite large~\citep{hornik89a,baum1989size}.\todo{expand here! put some math}
    
    This however did not match empirical results. Later work showed that rather than capacity being based on solely the number of weights, instead the number of large weights was a better measure~\citep{bartlett1997}
    
	\section{Network Architecture}
    A persistent question in training artificial neural networks has been in the design of the networks. Specifically the question of how many parameters should be learned, and in what way they should be connected,  so as to be suitable for good generalization from a given size dataset. Notable steps in the theoretical answers to this question include findings showing the limitations of single layer networks~\citep{minsky1988perceptrons}, information-theoretic measures of the representational capacity of a network~\citep{vapnik2015uniform}, the proof that single hidden-layer networks are universal approximators~\citep{hornik89a}, and the theoretical number of nodes required for generalization from a dataset of given size~\citep{baum1989size}. 
    
    Empirical results have, however, shown that the realities of training neural networks do not match what theory predicts. Deep networks of many hidden layers have been shown time and again to out-perform shallow networks~\citep{Krizhevsky2012,Simonyan2014verydeep,He2015,He2016}, perhaps due to our limited method of optimization~\citep{NIPS2014_5484}. Networks with many more parameters than training samples, that use early-stopping or are regularized strongly, generalize better in practice than networks with the theoretically sufficient capacity~\citep{caruana2001overfitting, Krizhevsky2012, HintonTalk2015}. Networks designed with a specialized connectivity structure closer reflecting the underlying solution have consistently generalized better than fully-connected networks with higher learning capacity~\citep{lecun1989backpropagation,He2016}. In fact these seemingly theoretically defying design strategies can claim to have been responsible for recent breakthroughs in generalization on previously difficult tasks such as image class recognition~\citep{Krizhevsky2012, HintonTalk2015}.
    
    \todo{Include references to \citet{caruana2001overfitting, baum1989size, rethinking2016}}
	
    \subsection{Model Size}
	\begin{chapquote}{John Denker \etal, \textit{Large Automatic Learning, Complex Systems,1987}}
		``A general tabula rasa network is a fine subject for the abstract, formal studies, but one should not try to use it to solve practical problems. \ldots One should pre-program the network with all available information about the structure of the problem, especially information about the symmetry and topology of the data.''
	\end{chapquote}
	\begin{figure}[tb]
		\centering
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit3rd.pgf}}
			\caption{\engordnumber{3}-order polynomial fitting 3 points}
			\label{fig:polyfit3rd}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit20th.pgf}}
			\caption{\engordnumber{20}-order polynomial fitting 3 points}
			\label{fig:polyfit20th}
		\end{subfigure}\\	
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit3rdlots.pgf}}
			\caption{\engordnumber{3}-order polynomial fitting 10 points}
			\label{fig:polyfit3rdlots}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit20thlots.pgf}}
			\caption{\engordnumber{20}-order polynomial fitting 10 points}
			\label{fig:polyfit20thlots}
		\end{subfigure}
		\caption[Polynomial Data Fitting]{Polynomial fits of samples from a 3rd order function. Polynomials of high order, like neural networks of many parameters, easily overfit a small number of samples as compared to polynomials of a more suitable order for the sampled function. While generalization is helped by more data, the higher order polynomial still tends to overfit.}
		\label{fig:polyfits}
	\end{figure}

	\citet{denker1987large} explored the relationship of network architecture to generalization. The work was particularly motivating in the later design of convolutional neural networks~\citep{lecun1989generalization, lecun1989backpropagation}. The authors make the intuitive analogy between the affect of the size of a neural network on its generalization, and a simple least-squares polynomial fit. Fig.~\ref{fig:polyfits} shows various polynomial fits to samples from a \engordnumber{3}-order polynomial function. When using a \engordnumber{3}-order polynomial to fit even a small number of samples (Fig.~\ref{fig:polyfit3rd}), the fit extrapolates, \ie is closer to the desired function outside the range of training samples, better than when we use a \engordnumber{20}-order polynomial to fit the same data (Fig.~\ref{fig:polyfit20th}). While the number of samples can help the fit of the higher order function, even with a large number of samples the \engordnumber{20}-order polynomial fit  (Fig.~\ref{fig:polyfit20thlots}) will not extrapolate as well as the polynomial with a more appropriate lower number of parameters (Fig.~\ref{fig:polyfit3rdlots}). Similarly, a neural network with a large number of parameters may not generalize as well as a neural network with fewer more salient parameters.
    
    \citet{caruana2001overfitting} further explored the analogy by training neural networks to fit polynomials, showing that overfitting in neural networks does not seem to be as serious a problem as in polynomials. The greatly over-parameterized neural networks still found relatively good fits. The authors suggest that neural networks trained with backpropogration may be biased towards ``smoother approximations''.
    
    \section{Deep Networks}
	% deep boltzmann networks
	\citep{Krizhevsky2012}
	\citep{Simonyan2014verydeep}
	\citep{He2015}
	\citep{He2016}
    
	
	%% NOTE: Polynomial fits are a special case of linear regression where we've used a polynomial basis
	%% Use example of sin wave? We can fit it with a polynomial, but better to reparameterize into a basis
	
	\subsection{Generalization and Parameters in Neural Networks}
	\begin{figure}[tb]
		\centering
		\large
        \renewcommand{\ttdefault}{pcr}
		\begin{subfigure}[t]{0.45\textwidth}
			\begin{center}
			\texttt{00\textbf{11111}00}\\
			\texttt{00\textbf{111}0000}\\
			\texttt{0000\textbf{1}0000}
			\end{center}
			\caption{Binary sequences with one clump}
			\label{fig:oneclump}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.45\textwidth}
			\begin{center}
			\texttt{00\textbf{11}0\textbf{11}00}\\
			\texttt{00\textbf{1}0\textbf{1}0\textbf{1}00}\\
			\texttt{00\textbf{111}0\textbf{1}00}
			\end{center}
			\caption{Binary sequences with two or more clumps}
			\label{fig:polyfit20th}
		\end{subfigure}
		
        \renewcommand{\ttdefault}{lmodern}
		\caption[Two-or-more Clump Predicate]{The two-or-more clumps predicate asks for the network to classify (padded) binary input sequences as having one or two or more contiguous strings of ones.}
		\label{fig:tomclumps}
	\end{figure}
	
	\citet{denker1987large} explore the relationship between network architecture and generalization by evaluating networks for solving the \emph{two-or-more clumps} predicate. The two-or-more clumps predicate asks for the network to classify binary input sequences as having one or two or more contiguous strings of ones, some examples of which are shown in Fig.~\ref{fig:tomclumps}. 
	
	The authors illustrate some surprising properties of the generalization of fully-connected neural networks learned with backpropogation. First, a human-preferred `geometric' solution is manually hard-coded into the weights of a fully-connected network. While this weight configuration is a valid solution, and is intuitive to humans, the authors show that is is not a solution that the network would ever settle upon when trained with backpropogation. By using the geometric solution as an initialization, and training the network further, the authors show that the error-surface around the region is not stable,
	
\section{The Neocognitron}
	\begin{chapquote}{Kunihiko Fukushima, \textit{Neocognitron, 
				%A Self-organizing Neural Network Model
				%for a Mechanism of Pattern Recognition
				%Unaffected by Shift in Position , 
				Biol.\ Cybernetics 1980}}
		`` One of the largest and long-standing difficulties in designing a pattern-recognizing machine has been the problem how to cope with the shift in position and the distortion in shape of the input patterns. The neocognitron proposed in this paper gives a drastic solution to this difficulty.
		''
	\end{chapquote}
	
	In an era in which fully connected networks were used to learn any input type, \citet{Fuk80} showed that for structured inputs a drastically different architecture could make a big difference in generalization. The \emph{Neocognitron}~\citep{Fuk80, fukushima2013artificial} was a biologically motivated architecture, motivated by what are typically called simple and complex cells in the primary visual cortex (V1), as found by \citet{Hubel1959a}. To model simple cells; cells whose response correlated with simple oriented edges in a translation invariant manner, the neocognitron used shared weights which were connected to local image patches of the input image (and were not simply described as convolution of a filter). Complex cells were modelled by a ``blurring'' operation, what we now term more generally as \emph{pooling}. The neocognitron network consisted of alternating layers of simple and complex cells, \ie alternating convolution and pooling layers, much as seen in state of the art convolutional networks.

	While the Neocognitron was ahead of its time, and is now recognized as the first iteration of what were to become convolutional neural networks, the article's neurological focus, timing, and to some degree country of origin, meant that it was somewhat unnoticed in the mainstream field of connectionist research. In fact Yann LeCun recounts specifically his interest in Japan, rather than any particular citation, having lead to his discovery of the work\footnote{As related in a Q\&A session at the 2016 International Computer Vision Summer School (ICVSS)}.
	
	\section{Convolutional Neural Networks}
	\begin{chapquote}{Yann LeCun, \textit{Backpropagation Applied to Handwritten Zip Code Recognition, 1989}}
		``Classical work in visual pattern recognition has demonstrated the advantage of extracting local features and combining them to form higher order features. Such knowledge can be easily built into the network by forcing the hidden units to combine only local sources of information. Distinctive features of an object can appear at various location on the input image. Therefore it seems judicious to have a set of feature detectors that can detect a particular instance of a feature anywhere on the input place. Since the precise location of a feature is not relevant to the classification, we can afford to lose some position information in the process. Nevertheless, approximate position information must be preserved, to allow the next levels to detect higher order, more complex features.''
	\end{chapquote}
	
	Despite the pioneering novelty of the work on neocognitrons, it was only following the simplification and improvement of~\citet{lecun1989backpropagation,Lecun1998} in both the description of the network and its operation that it gained wider acknowledgement as a breakthrough for image recognition. In their work the local shared weights of the neocognitron are put in the context of convolution, and the averaging operation replaces with max-pooling. The application to handwritten digit recognition gave state of the art results, and would result in the \emph{LeNet5} network, still used today in commercial applications.
	
	The application of the LeNet-style CNN architecture to more complex problems, however, proved infeasible. These problems required a deeper hierarchy of representation, which implied a large number of layers. Networks with a large number of layers proved to be un-trainable due to numerous issues with the model itself, notably vanishing gradients~\citep{hochreiter1991untersuchungen}, and the lack of large datasets and computational power at the time. Convolutional neural networks fell out of favour, and were passed over in favour of the more successful paradigm of using hand-crafted local features for many tasks, and in particular the problem of object instance recognition was well addressed by such solutions. Meanwhile object class recognition remained a difficult problem, for which the best solutions were deformable parts models, also based on local features.
	
	
	\section{Learning Neural Network Architectures}
	\begin{chapquote}{David MacKay, \textit{A Practical Bayesian Framework for Backprop Networks, Neural Computation 1991}}
		``There are many knobs on the black box of 'backprop' (learning by back-propagation of
		errors). Generally these knobs are set by rules of thumb, trial and error, and the use of reserved test data to assess generalisation ability (or more sophisticated cross-validation).''
	\end{chapquote}
	
	\mynote{todo talk about tiling, and MaKay's work}
	\citet{mezard1989learning,MacKay91,mackay1992practical}
	
	\section{Network Pruning, Compression and Quantization}
	\mynote{todo}
	Another approach to learning efficient networks is pruning
	\citet{lecun1989optimal,sietsma1988neural,Xing2009,journals/corr/HanMD15,ullrich2017soft,}
	
	
	\subsection{Structural Priors}
	In practice when fitting a curve, we have little idea of what order polynomial would best fit the data. Necessarily, we must use a relatively higher order curve to fit the data. Similarly, with a neural network we rarely have knowledge of the underlying structure of the solution (but when we do, we should use it to parameterize our models appropriately~\citep{jain2016structural}), instead we must use networks with more parameters than necessary to ensure that there is enough capacity to learn the underlying, but potentially sparse solution. Over-parameterization of a model generally leads to poor generalization however, due to overfitting. To prevent this, there are a number of methods in which we can relate our prior knowledge that the model is over-parameterized to the optimization:
	
	\begin{description}
	\item[Weak Priors]
	Knowing only that our model is over-parameterized is a relatively weak prior, however we can encode this into the fit by using a regularization penalty. This restricts the model to effectively use only a small number of the parameters by adding a penalty, for example on the L2 norm of the model weights. For polynomial regression, this is called \emph{ridge regression}, while for neural networks it is called \emph{weight decay}.
	
	\item[Strong Priors]
	With more prior information on the task, \eg from the convexity of the polynomial, we may imply that a certain order polynomial is more appropriate, and restrict learning to that order. For example, given samples from a polynomial appear to be convex, we can surmise that the polynomial is likely to be of an even or \engordnumber{2}-order, and restrict our fit to be of that order. 
    \end{description}
    
	Although neural networks are usually posed as general learning machines, time and again it has been demonstrated that neural networks only truly stand out as a learning method when we use strong structural priors, encoding our prior knowledge of the task in the architecture itself. As observed by \citet{denker1987large} this may be considered closer to modifying the problem to be solved itself, rather than changing the learning method alone. For example, by asking a CNN to learn to classify a dataset, we are asking the network to ``Classify these types of images'', whereas by asking a fully-connected network to classify the same dataset, we are asking the network ``Classify these types of data''. The first task is inherently easier.

    Structural priors may be considered one of the greatest contributions to the success of deep learning, but arguably can also be considered the cause of its greatest failure. Structural priors allow deep models to more easily learn specific tasks, \eg image classification or speech recognition. At the same time, by specializing our network models, we are moving further away from the goal of general artificial intelligence, \ie learnign models that can do both tasks.
\end{document}
