\documentclass[thesis]{subfiles}

\begin{document}
	\chapter{Motivation}
	\label{motivation}
	\begin{chapquote}{Yann LeCun, \textit{Backpropagation Applied to Handwritten Zip Code Recognition, 1989}}
		``Classical work in visual pattern recognition has demonstrated the advantage of extracting local features and combining them to form higher order features. Such knowledge can be easily built into the network by forcing the hidden units to combine only local sources of information. Distinctive features of an object can appear at various location on the input image. Therefore it seems judicious to have a set of feature detectors that can detect a particular instance of a feature anywhere on the input place. Since the precise location of a feature is not relevant to the classification, we can afford to lose some position information in the process. Nevertheless, approximate position information must be preserved, to allow the next levels to detect higher order, more complex features.''
	\end{chapquote}
	
	\begin{chapquote}{David MacKay, \textit{A Practical Bayesian Framework for Backprop Networks, Neural Computation 1991}}
	``There are many knobs on the black box of 'backprop' (learning by back-propagation of
	errors (Rumelhart et. al., 1986)). Generally these knobs are set by rules of thumb, trial and
	error, and the use of reserved test data to assess generalisation ability (or more sophisti-
	cated cross-validation).''
	\end{chapquote}
	
	A concept taken much for granted in the neural network community is that of neural network design. It is well known that the design of a neural network architecture can have a large effect on the generalization of that network when trained on a dataset. And yet, despite this, network design remains somewhat of a black magic, with intuition and experience being the cited motivation behind most common architectures, rather than any theory. This more than perhaps any other factor has been a barrier to access for the field.
	
	Beyond the simpler hyper-parameters, such as 
	
	Nowhere is this effect more pronounced than in the case of using neural networks with image inputs, which pose unique challenges in learning. As noted by \cite{Fuk80}, 
	
	\begin{chapquote}{Kunihiko Fukushima, \textit{Neocognitron, 
				%A Self-organizing Neural Network Model
				%for a Mechanism of Pattern Recognition
				%Unaffected by Shift in Position , 
				Biol.\ Cybernetics 1980}}
	`` One of the largest and long-standing difficulties in designing a pattern-recognizing machine has been the problem how to cope with the shift in position and the distortion in shape of the input patterns. The neocognitron proposed in this paper gives a drastic solution to this difficulty.''
	\end{chapquote}
	
	\section{The Unreasonable affect of Network Structure on Learning}
	
\end{document}
