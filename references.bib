@INPROCEEDINGS{Ioannou2016,
  AUTHOR = {Ioannou, Yani and Robertson, Duncan P and Shotton, Jamie and Cipolla, Roberto and Criminisi, Antonio},
  BOOKTITLE = {{International Conference on Learning Representations}},
  TITLE = {{Training CNNs with Low-Rank Filters for Efficient Image Classification}},
  YEAR = {2016},
}

@INPROCEEDINGS{ioannou2016e,
  ABSTRACT = {We propose a new method for training computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. Our sparse connection structure facilitates a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less compute, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40{\%} fewer parameters, 45{\%} fewer floating point operations, and is 31{\%} (12{\%}) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25{\%} fewer floating point operations and 44{\%} fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7{\%} fewer parameters and is 21{\%} (16{\%}) faster on a CPU (GPU).},
  ARXIVID = {1605.06489},
  AUTHOR = {Ioannou, Yani and Robertson, Duncan and Cipolla, Roberto and Criminisi, Antonio},
  BOOKTITLE = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017, Honolulu, Hawaii}},
  EPRINT = {1605.06489},
  EPRINTTYPE = {arXiv},
  TITLE = {{Deep Roots: Improving CNN efficiency with hierarchical filter groups}},
  YEAR = {2016},
}

@REPORT{Ioannou2015,
  AUTHOR = {Ioannou, Yani and Robertson, Duncan and Zikic, Darko and Kontschieder, Peter and Shotton, Jamie and Brown, Matthew and Criminisi, Antonio},
  BOOKTITLE = {{Technical Report}},
  INSTITUTION = {Microsoft Research},
  MONTH = {04},
  NUMBER = {MSR-TR-2015-58},
  TITLE = {{Decision Forests, Convolutional Networks and the Models in-Between}},
  TYPE = {techreport},
  YEAR = {2015},
}

@ARTICLE{journals/mcss/Cybenko92,
  AUTHOR = {Cybenko, G},
  JOURNALTITLE = {Mathematics of control, signals, and systems},
  KEYWORDS = {dblp},
  NUMBER = {4},
  PAGES = {303--314},
  TITLE = {{Approximation by superpositions of a sigmoid function}},
  URL = {http://dblp.uni-trier.de/db/journals/mcss/mcss5.html{\#}Cybenko92},
  VOLUME = {2},
  YEAR = {1989},
}

@ARTICLE{hornik89a,
  ABSTRACT = {Thesis BIB},
  AUTHOR = {Hornik, K and Stinchcombe, M and White, H},
  JOURNALTITLE = {Neural Networks},
  KEYWORDS = {imported},
  PAGES = {356--366},
  TITLE = {{Multilayer feedforward networks are universal approximators}},
  VOLUME = {2},
  YEAR = {1989},
}

@ARTICLE{Lecun1998,
  ABSTRACT = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
  AUTHOR = {Lecun, Y and Bottou, L and Bengio, Y and Haffner, P},
  ISSN = {0018-9219},
  JOURNALTITLE = {Proceedings of the IEEE},
  KEYWORDS = {2D GTN; back-propagation; backpropagation; based c},
  NUMBER = {11},
  PAGES = {2278--2324},
  TITLE = {{Gradient-based learning applied to document recognition}},
  VOLUME = {86},
  YEAR = {1998},
}

@BOOK{goodfellow2016deep,
  AUTHOR = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  PUBLISHER = {MIT press},
  TITLE = {{Deep learning}},
  YEAR = {2016},
}

@BOOK{Bishop1995,
  AUTHOR = {Bishop, Christopher M},
  KEYWORDS = {imported},
  LOCATION = {Oxford},
  PUBLISHER = {Oxford University Press},
  TITLE = {{Neural Networks for Pattern Recognition}},
  YEAR = {1995},
}

@ARTICLE{rosenblatt1958perceptron,
  AUTHOR = {Rosenblatt, Frank},
  JOURNALTITLE = {Psychological review},
  NUMBER = {6},
  PAGES = {386},
  PUBLISHER = {American Psychological Association},
  TITLE = {{The perceptron: A probabilistic model for information storage and organization in the brain.}},
  VOLUME = {65},
  YEAR = {1958},
}

@BOOK{minsky1988perceptrons,
  AUTHOR = {Minsky, Marvin and Papert, Seymour},
  PUBLISHER = {MIT press},
  TITLE = {{Perceptrons}},
  YEAR = {1988},
}

@REPORT{rosenblatt1961principles,
  AUTHOR = {Rosenblatt, Frank},
  INSTITUTION = {CORNELL AERONAUTICAL LAB INC BUFFALO NY},
  TITLE = {{Principles of neurodynamics. perceptrons and the theory of brain mechanisms}},
  TYPE = {techreport},
  YEAR = {1961},
}

@REPORT{widrow1960adaptive,
  AUTHOR = {Widrow, Bernard and Hoff, Marcian E},
  INSTITUTION = {Stanford University},
  TITLE = {{Adaptive switching circuits}},
  TYPE = {techreport},
  YEAR = {1960},
}

@ARTICLE{rumelhartbackprop,
  AUTHOR = {{Rumelhart David E.} and {Hinton Geoffrey E.} and {Williams Ronald J.}},
  DOI = {10.1038/323533a0},
  JOURNALTITLE = {Nature},
  MONTH = {10},
  NOTE = {10.1038/323533a0},
  NUMBER = {6088},
  PAGES = {533--536},
  RISFIELD_0_M3 = {10.1038/323533a0},
  TITLE = {{Learning representations by back-propagating errors}},
  VOLUME = {323},
  YEAR = {1986},
}

@BOOK{haykin1994neural,
  AUTHOR = {Haykin, Simon},
  PUBLISHER = {Prentice Hall PTR},
  TITLE = {Neural networks: a comprehensive foundation},
  YEAR = {1994},
}

@INCOLLECTION{Bottou2012sgdtricks,
  AUTHOR = {Bottou, Léon},
  BOOKTITLE = {{Neural Networks: Tricks of the Trade (2nd ed.)}},
  EDITOR = {Montavon, Grégoire and Orr, Genevieve B and Müller, Klaus-Robert},
  ISBN = {978-3-642-35288-1},
  KEYWORDS = {dblp},
  PAGES = {421--436},
  PUBLISHER = {Springer},
  SERIES = {{Lecture Notes in Computer Science}},
  TITLE = {{Stochastic Gradient Descent Tricks.}},
  VOLUME = {7700},
  YEAR = {2012},
}

@INPROCEEDINGS{martens2010deep,
  AUTHOR = {Martens, James},
  BOOKTITLE = {{Proceedings of the 27th International Conference on Machine Learning (ICML-10)}},
  PAGES = {735--742},
  TITLE = {{Deep learning via Hessian-free optimization}},
  YEAR = {2010},
}

@ARTICLE{polyak1964some,
  AUTHOR = {Polyak, Boris T},
  JOURNALTITLE = {USSR Computational Mathematics and Mathematical Physics},
  NUMBER = {5},
  PAGES = {1--17},
  PUBLISHER = {Elsevier},
  TITLE = {Some methods of speeding up the convergence of iteration methods},
  VOLUME = {4},
  YEAR = {1964},
}

@ARTICLE{Fuk80,
  AUTHOR = {Fukushima, K},
  JOURNALTITLE = {Biological Cybernetics},
  KEYWORDS = {deep fukushima learning neocognitron networks neur},
  PAGES = {193--202},
  TITLE = {{Neocognitron: A self-organizing neural network model for a mechanish of pattern recognition unaffected by shifts in position}},
  VOLUME = {36},
  YEAR = {1980},
}

@ARTICLE{fukushima2013artificial,
  ABSTRACT = {The neocognitron is a neural network model proposed by. Fukushima (1980). Its architecture was suggested by neurophysiological findings on the visual systems of mammals. It is a hierarchical multi-layered network. It acquires the ability to robustly recognize visual patterns through learning. Although the neocognitron has a long history, modifications of the network to improve its performance are still going on. For example, a recent neocognitron uses a new learning rule, named add-if-silent, which makes the learning process much simpler and more stable. Nevertheless, a high recognition rate can be kept with a smaller scale of the network. Referring to the history of the neocognitron, this paper discusses recent advances in the neocognitron. We also show that various new functions can be realized by, for example, introducing top-down connections to the neocognitron: mechanism of selective attention, recognition and completion of partly occluded patterns, restoring occluded contours, and so on. {©} 2012 Elsevier Ltd.},
  AUTHOR = {Fukushima, Kunihiko},
  DOI = {10.1016/j.neunet.2012.09.016},
  ISSN = {08936080},
  JOURNALTITLE = {Neural Networks},
  KEYWORDS = {Artificial vision; Bottom-up and top-down; Hierarchical network; Modeling neural networks; Neocognitron},
  PAGES = {103--119},
  PMID = {23098752},
  PUBLISHER = {Elsevier},
  TITLE = {{Artificial vision by multi-layered neural networks: Neocognitron and its advances}},
  VOLUME = {37},
  YEAR = {2013},
}

@THESIS{hochreiter1991untersuchungen,
  AUTHOR = {Hochreiter, Sepp},
  BOOKTITLE = {{Diploma, Technische Universit{\{}ä{\}}t M{\{}ü{\}}nchen}},
  INSTITUTION = {Technische Universit{\{}ä{\}}t M{\{}ü{\}}nchen},
  PAGES = {91},
  TITLE = {{Untersuchungen zu dynamischen neuronalen Netzen}},
  TYPE = {phdthesis},
  YEAR = {1991},
}

@BOOK{damelin2011,
  ABSTRACT = {Arising from courses taught by the authors, this largely self-contained treatment is ideal for mathematicians who are interested in applications or for students from applied fields who want to understand the mathematics behind their subject. Early chapters cover Fourier analysis, functional analysis, probability and linear algebra, all of which have been chosen to prepare the reader for the applications to come. The book includes rigorous proofs of core results in compressive sensing and wavelet convergence. Fundamental is the treatment of the linear system y=$\Phi$x in both finite and infinite dimensions. There are three possibilities: the system is determined, overdetermined or underdetermined, each with different aspects. The authors assume only basic familiarity with advanced calculus, linear algebra and matrix theory and modest familiarity with signal processing, so the book is accessible to students from the advanced undergraduate level. Many exercises are also included.},
  AUTHOR = {Damelin, Steven B. and {Miller Jr}, Willard},
  DOI = {10.1017/CBO9781139003896},
  ISBN = {9781107601048},
  LOCATION = {Cambridge},
  PAGES = {462},
  PMID = {17238176},
  PUBLISHER = {Cambridge University Press},
  TITLE = {{The Mathematics of Signal Processing}},
  URL = {http://www.amazon.com/Mathematics-Signal-Processing-Cambridge-Applied/dp/1107601045/ref=pd{\_}sim{_}sbs{_}b{_}4?ie=UTF8{&}refRID=0TKKM2SWXXJPAXKE5KWG},
  YEAR = {2012},
}

@ARTICLE{He2015,
  ABSTRACT = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
  ARXIVID = {1512.03385},
  AUTHOR = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  DOI = {10.3389/fpsyg.2013.00124},
  EPRINT = {1512.03385},
  EPRINTTYPE = {arXiv},
  ISBN = {978-1-4673-6964-0},
  ISSN = {1664-1078},
  JOURNALTITLE = {Arxiv.Org},
  KEYWORDS = {deep learning; denoising auto-encoder; image denoising},
  NUMBER = {3},
  PAGES = {171--180},
  PMID = {23554596},
  TITLE = {{Deep Residual Learning for Image Recognition}},
  URL = {http://arxiv.org/pdf/1512.03385v1.pdf},
  VOLUME = {7},
  YEAR = {2015},
}

@INPROCEEDINGS{glorot2010understanding,
  ABSTRACT = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  AUTHOR = {Glorot, Xavier and Bengio, Yoshua},
  BOOKTITLE = {{Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)}},
  ISSN = {15324435},
  PAGES = {249--256},
  TITLE = {{Understanding the difficulty of training deep feedforward neural networks}},
  URL = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{_}GlorotB10.pdf},
  VOLUME = {9},
  YEAR = {2010},
}

@INPROCEEDINGS{conf/icml/NairH10,
  ABSTRACT = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ``Stepped Sigmoid Units'' are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
  AUTHOR = {Nair, Vinod and Hinton, Geoffrey E},
  BOOKTITLE = {{Proceedings of the 27th International Conference on Machine Learning}},
  EDITOR = {Fürnkranz, Johannes and Joachims, Thorsten},
  ISBN = {9781605589077},
  ISSN = {1935-8237},
  KEYWORDS = {dblp},
  NUMBER = {3},
  PAGES = {807--814},
  PMID = {22404682},
  PUBLISHER = {Omnipress},
  TITLE = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
  URL = {http://dblp.uni-trier.de/db/conf/icml/icml2010.html{\#}NairH10},
  YEAR = {2010},
}

@INPROCEEDINGS{Krizhevsky2012,
  ABSTRACT = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
  ARXIVID = {1102.0183},
  AUTHOR = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  BOOKTITLE = {{Advances In Neural Information Processing Systems}},
  DOI = {10.1016/j.protcy.2014.09.007},
  EPRINT = {1102.0183},
  EPRINTTYPE = {arXiv},
  TITLE = {{ImageNet Classification with Deep Convolutional Neural Networks}},
  YEAR = {2012},
}

@ARTICLE{hinton2006reducing,
  ABSTRACT = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural$\backslash$rnetwork with a small central layer to reconstruct high-dimensional input vectors. Gradient descent$\backslash$r$\backslash$ncan be used for fine-tuning the weights in such {\lq}{\lq}autoencoder'' networks, but this works well only if$\backslash$r$\backslash$nthe initial weights are close to a good solution. We describe an effective way of initializing the$\backslash$r$\backslash$nweights that allows deep autoencoder networks to learn low-dimensional codes that work much$\backslash$r$\backslash$nbetter than principal components analysis as a tool to reduce the dimensionality of data.$\backslash$r$\backslash$n},
  ARXIVID = {20},
  AUTHOR = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  DOI = {10.1126/science.1127647},
  EPRINT = {20},
  EPRINTTYPE = {arXiv},
  ISSN = {1095-9203},
  JOURNALTITLE = {Science},
  NUMBER = {5786},
  PAGES = {504--507},
  PMID = {16873662},
  PUBLISHER = {American Association for the Advancement of Science},
  TITLE = {{Reducing the Dimensionality of Data with Neural Networks$\backslash$r}},
  VOLUME = {313},
  YEAR = {2006},
}

@INPROCEEDINGS{He2015b,
  ABSTRACT = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94{\%} top-5 test error on the ImageNet 2012 clas-sification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [33]). To our knowledge, our result is the first 1 to surpass the reported human-level performance (5.1{\%}, [26]) on this dataset.},
  ARXIVID = {1502.01852},
  AUTHOR = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  BOOKTITLE = {{IEEE Conference on Computer Vision and Patern Recognition (ICCV)}},
  DOI = {10.1109/ICCV.2015.123},
  EPRINT = {1502.01852},
  EPRINTTYPE = {arXiv},
  ISBN = {978-1-4673-8391-2},
  ISSN = {15505499},
  KEYWORDS = {dblp},
  PAGES = {1026--1034},
  PUBLISHER = {IEEE},
  TITLE = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
  YEAR = {2015},
}

@INPROCEEDINGS{Ioffe2015,
  AUTHOR = {Ioffe, Sergey and Szegedy, Christian},
  BOOKTITLE = {{Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015}},
  TITLE = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.}},
  YEAR = {2015},
}

@ARTICLE{bengio:ieeenn94,
  AUTHOR = {Bengio, Yoshua and Simard, Patrick and Frasconi, Paolo},
  JOURNALTITLE = {IEEE Transactions on Neural Networks},
  KEYWORDS = {nn},
  NUMBER = {2},
  PAGES = {157--166},
  TITLE = {{Learning Long-Term Dependencies With Gradient Descent Is Difficult}},
  VOLUME = {5},
  YEAR = {1994},
}

@ARTICLE{ILSVRC2015,
  AUTHOR = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
  DOI = {10.1007/s11263-015-0816-y},
  JOURNALTITLE = {International Journal of Computer Vision (IJCV)},
  TITLE = {{ImageNet Large Scale Visual Recognition Challenge}},
  YEAR = {2015},
}

@INPROCEEDINGS{Sutskever2013momentum,
  AUTHOR = {Sutskever, Ilya and Martens, James and Dahl, George E and Hinton, Geoffrey E},
  BOOKTITLE = {{Proceedings of the 30th International Conference on Machine Learning, {\{}ICML{\}} 2013, Atlanta, GA, USA, 16-21 June 2013}},
  PAGES = {1139--1147},
  PUBLISHER = {JMLR.org},
  SERIES = {{{\{}JMLR{\}} Workshop and Conference Proceedings}},
  TITLE = {{On the importance of initialization and momentum in deep learning}},
  URL = {http://jmlr.org/proceedings/papers/v28/sutskever13.html},
  VOLUME = {28},
  YEAR = {2013},
}

@MISC{Hinton2012,
  ABSTRACT = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  ARXIVID = {1207.0580},
  AUTHOR = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  BOOKTITLE = {{ArXiv e-prints}},
  EPRINT = {1207.0580},
  EPRINTTYPE = {arXiv},
  ISBN = {9781467394673},
  ISSN = {9781467394673},
  MONTH = {07},
  PAGES = {1--18},
  PMID = {1000104337},
  TITLE = {{Improving neural networks by preventing co-adaptation of feature detectors}},
  URL = {http://arxiv.org/abs/1207.0580},
  YEAR = {2012},
}

@ARTICLE{Lin2013NiN,
  ABSTRACT = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  ARXIVID = {1312.4400},
  AUTHOR = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  DOI = {10.1109/ASRU.2015.7404828},
  EPRINT = {1312.4400},
  EPRINTTYPE = {arXiv},
  ISBN = {9781479972913},
  JOURNALTITLE = {arXiv preprint},
  KEYWORDS = {In Network},
  PAGES = {10},
  TITLE = {{Network In Network}},
  URL = {http://arxiv.org/abs/1312.4400},
  VOLUME = {abs/1312.4},
  YEAR = {2013},
}

@INPROCEEDINGS{Simonyan2014verydeep,
  ABSTRACT = {Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) ...},
  AUTHOR = {Simonyan, K and Zisserman, A},
  BOOKTITLE = {{eprint ar{\{}X{\}}iv:arXiv:1409.1556v5}},
  TITLE = {{Very deep convolutional networks for large-scale image recognition}},
  YEAR = {2014},
}

@INPROCEEDINGS{Sermanet2013overfeat,
  ABSTRACT = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
  ARXIVID = {1312.6229},
  AUTHOR = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
  BOOKTITLE = {{arXiv preprint arXiv}},
  EPRINT = {1312.6229},
  EPRINTTYPE = {arXiv},
  PAGES = {1312.6229},
  TITLE = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
  URL = {http://arxiv.org/abs/1312.6229},
  YEAR = {2013},
}

@INPROCEEDINGS{Szegedy2014going,
  AUTHOR = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  BOOKTITLE = {{Computer Vision and Pattern Recognition (CVPR)}},
  TITLE = {{Going Deeper with Convolutions}},
  URL = {http://arxiv.org/abs/1409.4842},
  YEAR = {2015},
}

@ARTICLE{Lowe2004,
  ABSTRACT = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
  AUTHOR = {Lowe, David G},
  DOI = {10.1023/B:VISI.0000029664.99615.94},
  FILE = {:home/yani/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe - 2004 - Distinctive Image Features from Scale-Invariant Keypoints.pdf:pdf},
  ISBN = {1568811012},
  ISSN = {09205691},
  JOURNALTITLE = {International Journal of Computer Vision},
  NUMBER = {2},
  PAGES = {91--110},
  PMID = {20064111},
  PUBLISHER = {Springer},
  SERIES = {{Int. J. Comput. Vis. (Netherlands)}},
  TITLE = {{Distinctive Image Features from Scale-Invariant Keypoints}},
  URL = {http://www.springerlink.com/openurl.asp?id=doi:10.1023/B:VISI.0000029664.99615.94},
  VOLUME = {60},
  YEAR = {2004},
}

@BOOK{hebb1949organization,
  AUTHOR = {Hebb, Donald Olding},
  PUBLISHER = {John Wiley \& Sons},
  TITLE = {{The organization of behavior: A neuropsychological approach}},
  YEAR = {1949},
}

@INCOLLECTION{vapnik2015uniform,
  AUTHOR = {Vapnik, Vladimir N and Chervonenkis, A Ya},
  BOOKTITLE = {{Measures of Complexity}},
  PAGES = {11--30},
  PUBLISHER = {Springer},
  TITLE = {{On the uniform convergence of relative frequencies of events to their probabilities}},
  YEAR = {2015},
}

@INPROCEEDINGS{baum1989size,
  AUTHOR = {Baum, Eric B and Haussler, David},
  BOOKTITLE = {{Advances in neural information processing systems}},
  PAGES = {81--90},
  TITLE = {{What size net gives valid generalization?}},
  YEAR = {1989},
}

@ARTICLE{He2016,
  ABSTRACT = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which further makes training easy and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10/100, and a 200-layer ResNet on ImageNet.},
  ARXIVID = {1603.05027},
  AUTHOR = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  EPRINT = {1603.05027},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {arXiv preprint},
  PAGES = {1--15},
  TITLE = {{Identity Mappings in Deep Residual Networks}},
  URL = {http://arxiv.org/abs/1603.05027},
  VOLUME = {abs/1603.0},
  YEAR = {2016},
}

@INCOLLECTION{NIPS2014_5484,
  AUTHOR = {Ba, Jimmy and Caruana, Rich},
  BOOKTITLE = {{Advances in Neural Information Processing Systems 27}},
  EDITOR = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  PAGES = {2654--2662},
  PUBLISHER = {Curran Associates, Inc.},
  TITLE = {{Do Deep Nets Really Need to be Deep?}},
  URL = {http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf},
  YEAR = {2014},
}

@INPROCEEDINGS{caruana2001overfitting,
  AUTHOR = {Caruana, Rich and Lawrence, Steve and Giles, C Lee},
  BOOKTITLE = {{Advances in neural information processing systems}},
  PAGES = {402--408},
  TITLE = {{Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping}},
  YEAR = {2001},
}

@MISC{HintonTalk2015,
  AUTHOR = {Hinton, Geoffery E.},
  HOWPUBLISHED = {Public Lecture},
  INSTITUTION = {Department of Engineering},
  LOCATION = {University of Cambridge},
  MONTH = {06},
  SERIES = {{Division F Talks}},
  TITLE = {{Deep Learning}},
  URL = {http://sms.cam.ac.uk/media/2017973},
  YEAR = {2015},
}

@ARTICLE{lecun1989backpropagation,
  AUTHOR = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  JOURNALTITLE = {Neural computation},
  NUMBER = {4},
  PAGES = {541--551},
  PUBLISHER = {MIT Press},
  TITLE = {{Backpropagation applied to handwritten zip code recognition}},
  VOLUME = {1},
  YEAR = {1989},
}

@ARTICLE{burges1998tutorial,
  AUTHOR = {Burges, Christopher JC},
  JOURNALTITLE = {Data mining and knowledge discovery},
  NUMBER = {2},
  PAGES = {121--167},
  PUBLISHER = {Springer},
  TITLE = {{A tutorial on support vector machines for pattern recognition}},
  VOLUME = {2},
  YEAR = {1998},
}

@ARTICLE{bartlett1997,
  AUTHOR = {Bartlett, Peter L},
  JOURNALTITLE = {Advances in neural information processing systems},
  PAGES = {134--140},
  PUBLISHER = {MORGAN KAUFMANN PUBLISHERS},
  TITLE = {{For valid generalization, the size of the weights is more important than the size of the network}},
  YEAR = {1997},
}

@INPROCEEDINGS{hinton1987learning,
  AUTHOR = {Hinton, GE and de Bakker, JW},
  BOOKTITLE = {{Proceedings of the Conference on Parallel Architectures and Languages Europe (PARLE)}},
  ORGANIZATION = {Springer},
  PAGES = {1--13},
  TITLE = {{Learning translation invariant recognition in a massively parallel networks}},
  YEAR = {1987},
}

@INPROCEEDINGS{rethinking2016,
  ARXIVID = {1611.03530},
  AUTHOR = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  EPRINT = {1611.03530},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {International Conference on Learning Representations (ICLR)},
  LOCATION = {Toulon, France},
  TITLE = {{Understanding deep learning requires rethinking generalization}},
  YEAR = {2017},
}

@ARTICLE{denker1987large,
  AUTHOR = {Denker, John and Schwartz, Daniel and Wittner, Ben and Solla, Sara and Howard, Richard and Jackel, Lawrence and Hopfield, John},
  JOURNALTITLE = {Complex systems},
  NUMBER = {5},
  PAGES = {877--922},
  TITLE = {{Large automatic learning, rule extraction, and generalization}},
  VOLUME = {1},
  YEAR = {1987},
}

@ARTICLE{lecun1989generalization,
  AUTHOR = {LeCun, Yann and others},
  JOURNALTITLE = {Connectionism in perspective},
  PAGES = {143--155},
  PUBLISHER = {Zurich, Switzerland: Elsevier},
  TITLE = {{Generalization and network design strategies}},
  YEAR = {1989},
}

@ARTICLE{giles1987learning,
  AUTHOR = {Giles, C Lee and Maxwell, Tom},
  JOURNALTITLE = {Applied optics},
  NUMBER = {23},
  PAGES = {4972--4978},
  PUBLISHER = {Optical Society of America},
  TITLE = {{Learning, invariance, and generalization in high-order neural networks}},
  VOLUME = {26},
  YEAR = {1987},
}

@ARTICLE{wolpert1996lack,
  AUTHOR = {Wolpert, David H},
  JOURNALTITLE = {Neural computation},
  NUMBER = {7},
  PAGES = {1341--1390},
  PUBLISHER = {MIT Press},
  TITLE = {{The lack of a priori distinctions between learning algorithms}},
  VOLUME = {8},
  YEAR = {1996},
}

@INCOLLECTION{lattimore2013no,
  AUTHOR = {Lattimore, Tor and Hutter, Marcus},
  BOOKTITLE = {{Algorithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence}},
  PAGES = {223--235},
  PUBLISHER = {Springer},
  TITLE = {{No free lunch versus Occam{\rq}s razor in supervised learning}},
  YEAR = {2013},
}

@ARTICLE{mezard1989learning,
  AUTHOR = {Mezard, Marc and Nadal, Jean-P},
  JOURNALTITLE = {Journal of Physics A: Mathematical and General},
  NUMBER = {12},
  PAGES = {2191},
  PUBLISHER = {IOP Publishing},
  TITLE = {{Learning in feedforward layered networks: The tiling algorithm}},
  VOLUME = {22},
  YEAR = {1989},
}

@INPROCEEDINGS{Fahlman1989,
  AUTHOR = {Fahlmann, S E and Lebiere, C},
  BOOKTITLE = {{Advances in Neural Information Processing Systems 2}},
  DOI = {10.1190/1.1821929},
  EDITOR = {Touretzky, David S},
  ISBN = {1558601007},
  ISSN = {10459227},
  PAGES = {524--532},
  PMID = {220943591},
  PUBLISHER = {Morgan Kaufmann},
  TITLE = {{The Cascade-Correlation Learning Architecture}},
  YEAR = {1990},
}

@ARTICLE{frean1990upstart,
  AUTHOR = {Frean, Marcus},
  JOURNALTITLE = {Neural computation},
  NUMBER = {2},
  OWNER = {yani},
  PAGES = {198--209},
  PUBLISHER = {MIT Press},
  TIMESTAMP = {2017.09.13},
  TITLE = {The upstart algorithm: A method for constructing and training feedforward neural networks},
  VOLUME = {2},
  YEAR = {1990},
}

@THESIS{MacKay91,
  AUTHOR = {MacKay, D. J. C.},
  INSTITUTION = {California Institute of Technology},
  TITLE = {{Bayesian Methods for Adaptive Models}},
  TYPE = {phdthesis},
  YEAR = {1991},
}

@ARTICLE{mackay1992practical,
  AUTHOR = {MacKay, David JC},
  JOURNALTITLE = {Neural computation},
  NUMBER = {3},
  PAGES = {448--472},
  TITLE = {{A practical Bayesian framework for backpropagation networks}},
  VOLUME = {4},
  YEAR = {1992},
}

@INPROCEEDINGS{hypernetworks,
  AUTHOR = {Ha, David and Dai, Andrew and Le, Quoc V.},
  BOOKTITLE = {{International Conference on Learning Representations (ICLR), Toulon, France}},
  TITLE = {{HyperNetworks}},
  YEAR = {2017},
}

@ARTICLE{parekh2000constructive,
  AUTHOR = {Parekh, Rajesh and Yang, Jihoon and Honavar, Vasant},
  JOURNALTITLE = {IEEE Transactions on neural networks},
  NUMBER = {2},
  PAGES = {436--451},
  PUBLISHER = {IEEE},
  TITLE = {{Constructive neural-network learning algorithms for pattern classification}},
  VOLUME = {11},
  YEAR = {2000},
}

@INPROCEEDINGS{lecun1989optimal,
  ABSTRACT = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
  ARXIVID = {arXiv:1011.1669v3},
  AUTHOR = {LeCun, Yann and Denker, John S and Solla, Sara a},
  BOOKTITLE = {{Advances in Neural Information Processing Systems}},
  EPRINT = {arXiv:1011.1669v3},
  EPRINTTYPE = {arXiv},
  ISBN = {1558601007},
  ISSN = {1098-6596},
  NUMBER = {1},
  PAGES = {598--605},
  PMID = {25246403},
  TITLE = {{Optimal Brain Damage}},
  VOLUME = {2},
  YEAR = {1990},
}

@INPROCEEDINGS{sietsma1988neural,
  AUTHOR = {Sietsma, Jocelyn and Dow, Robert JF},
  BOOKTITLE = {{IEEE International Conference on Neural Networks}},
  ORGANIZATION = {IEEE San Diego},
  PAGES = {325--333},
  TITLE = {{Neural net pruning-why and how}},
  VOLUME = {1},
  YEAR = {1988},
}

@ARTICLE{han2015deep,
  AUTHOR = {Han, Song and Mao, Huizi and Dally, William J},
  BOOKTITLE = {{International Conference on Learning Representations (ICLR), San Juan, Puerto Rico}},
  TITLE = {{Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding}},
  YEAR = {2016},
}

@INPROCEEDINGS{ullrich2017soft,
  ARXIVID = {1702.04008},
  AUTHOR = {Ullrich, Karen and Meeds, Edward and Welling, Max},
  BOOKTITLE = {{International Conference on Learning Representations}},
  EPRINTTYPE = {arXiv},
  TITLE = {{Soft Weight-Sharing for Neural Network Compression}},
  YEAR = {2017},
}

@INPROCEEDINGS{hanson1989comparing,
  AUTHOR = {Hanson, Stephen José and Pratt, Lorien Y},
  BOOKTITLE = {{Advances in neural information processing systems}},
  PAGES = {177--185},
  TITLE = {{Comparing biases for minimal network construction with back-propagation}},
  YEAR = {1989},
}

@INPROCEEDINGS{han2016dsd,
  AUTHOR = {Han, Song and Pool, Jeff and Narang, Sharan and Mao, Huizi and Gong, Enhao and Tang, Shijian and Elsen, Erich and Vajda, Peter and Paluri, Manohar and Tran, John and others},
  BOOKTITLE = {{International Conference on Learning Representations (ICLR), Toulon, France}},
  TITLE = {{Dsd: Dense-sparse-dense training for deep neural networks}},
  YEAR = {2017},
}

@ARTICLE{Hubel1959a,
  ABSTRACT = {Recordings were made in lightly anaesthesized cats whose retinas were stimulated, singly or together, with light spots of various sizes and shapes. Receptive fields, defined as restricted areas where illumination influenced the firing of a single cortical unit, usually contained mutually antagonistic excitatory and inhibitory regions. Thus a stimulus covering a whole field was relatively ineffective in driving most units. Effective driving of a unit required a stimulus specific in form, size, position, and orientation; based on the arrangement of excitatory and inhibitory areas. About 20{\%} of the cortical units studied could be activated from either eye; these were driven from roughly homologous regions of the retinas and summation and antagonism could be shown. (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
  AUTHOR = {Hubel, D H and Wiesel, T N},
  DOI = {10.1113/jphysiol.2009.174151},
  ISSN = {00223751},
  JOURNALTITLE = {Journal of Physiology},
  KEYWORDS = {CEREBRAL CORTEX/physiology; NEURONS/physiology},
  PAGES = {574--591},
  PMID = {14403679},
  TITLE = {{Receptive fields of single neurones in the cat's striate cortex.}},
  URL = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{&}id=14403679{&}retmode=ref{&}cmd=prlinks},
  VOLUME = {148},
  YEAR = {1959},
}

@INPROCEEDINGS{jain2016structural,
  AUTHOR = {Jain, Ashesh and Zamir, Amir R and Savarese, Silvio and Saxena, Ashutosh},
  BOOKTITLE = {{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}},
  PAGES = {5308--5317},
  TITLE = {{Structural-RNN: Deep learning on spatio-temporal graphs}},
  YEAR = {2016},
}

@ARTICLE{mozer1989using,
  AUTHOR = {Mozer, Michael C and Smolensky, Paul},
  JOURNALTITLE = {Connection Science},
  NUMBER = {1},
  OWNER = {yani},
  PAGES = {3--16},
  PUBLISHER = {Taylor \& Francis},
  TIMESTAMP = {2017.09.13},
  TITLE = {Using relevance to reduce network size automatically},
  VOLUME = {1},
  YEAR = {1989},
}

@INPROCEEDINGS{mozer1989skeletonization,
  AUTHOR = {Mozer, Michael C and Smolensky, Paul},
  BOOKTITLE = {Advances in neural information processing systems},
  OWNER = {yani},
  PAGES = {107--115},
  TIMESTAMP = {2017.09.13},
  TITLE = {Skeletonization: A technique for trimming the fat from a network via relevance assessment},
  YEAR = {1989},
}

@ARTICLE{castellano1997iterative,
  AUTHOR = {Castellano, Giovanna and Fanelli, Anna Maria and Pelillo, Marcello},
  JOURNALTITLE = {IEEE Transactions on Neural Networks},
  NUMBER = {3},
  OWNER = {yani},
  PAGES = {519--531},
  PUBLISHER = {IEEE},
  TIMESTAMP = {2017.09.13},
  TITLE = {{An iterative pruning algorithm for feedforward neural networks}},
  VOLUME = {8},
  YEAR = {1997},
}

@ARTICLE{gorodkin1993quantitative,
  AUTHOR = {Gorodkin, Jan and Hansen, Lars Kai and Krogh, Anders and Svarer, Claus and Winther, Ole},
  JOURNALTITLE = {International journal of neural systems},
  NUMBER = {02},
  OWNER = {yani},
  PAGES = {159--169},
  PUBLISHER = {World Scientific},
  TIMESTAMP = {2017.09.13},
  TITLE = {A quantitative study of pruning by optimal brain damage},
  VOLUME = {4},
  YEAR = {1993},
}

@INPROCEEDINGS{han2015learning,
  AUTHOR = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  OWNER = {yani},
  PAGES = {1135--1143},
  TIMESTAMP = {2017.09.13},
  TITLE = {{Learning both weights and connections for efficient neural network}},
  YEAR = {2015},
}

@ARTICLE{setiono1997penalty,
  AUTHOR = {Setiono, Rudy},
  JOURNALTITLE = {Neural computation},
  NUMBER = {1},
  OWNER = {yani},
  PAGES = {185--204},
  PUBLISHER = {MIT Press},
  TIMESTAMP = {2017.09.13},
  TITLE = {A penalty-function approach for pruning feedforward neural networks},
  VOLUME = {9},
  YEAR = {1997},
}

@INPROCEEDINGS{ahmad1989scaling,
  AUTHOR = {Ahmad, Subutai and Tesauro, Gerald},
  BOOKTITLE = {Advances in neural information processing systems},
  OWNER = {yani},
  PAGES = {160--168},
  TIMESTAMP = {2017.09.13},
  TITLE = {Scaling and generalization in neural networks: a case study},
  YEAR = {1989},
}

@INPROCEEDINGS{Kim2016,
  ABSTRACT = {Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.},
  ARXIVID = {1511.06530},
  AUTHOR = {Kim, Yong-Deok and Park, Eunhyeok and Yoo, Sungjoo and Choi, Taelim and Yang, Lu and Shin, Dongjun},
  BOOKTITLE = {{International Conference on Learning Representations (ICLR)}},
  EPRINT = {1511.06530},
  EPRINTTYPE = {arXiv},
  PAGES = {1--16},
  TITLE = {{Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications}},
  URL = {http://arxiv.org/abs/1511.06530},
  YEAR = {2016},
}

@ARTICLE{schwartz1990exhaustive,
  AUTHOR = {Schwartz, Daniel B and Samalam, Vijay K and Solla, Sara A and Denker, John S},
  JOURNALTITLE = {Neural Computation},
  NUMBER = {3},
  OWNER = {yani},
  PAGES = {374--385},
  PUBLISHER = {MIT Press},
  TIMESTAMP = {2017.09.13},
  TITLE = {Exhaustive learning},
  VOLUME = {2},
  YEAR = {1990},
}

@MISC{1502.02551v1,
  ABSTRACT = {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited preci- sion data representation and computation on neu- ral network training. Within the context of low- precision fixed-point computations, we observe the rounding scheme to play a crucial role in de- termining the network's behavior during train- ing. Our results show that deep networks can be trained using only 16-bit wide fixed-point num- ber representation when using stochastic round- ing, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that imple- ments low-precision fixed-point arithmetic with stochastic rounding.},
  ANNOTATION = {published = 2015-02-09T16:37:29Z, updated = 2015-02-09T16:37:29Z, 10 pages, 6 figures, 1 table},
  ARXIVID = {1502.02551},
  AUTHOR = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  BOOKTITLE = {{Proceedings of the 32nd International Conference on Machine Learning (ICML-15)}},
  DOI = {10.1109/72.80206},
  EPRINT = {1502.02551},
  EPRINTTYPE = {arXiv},
  ISBN = {9781510810587},
  ISSN = {19410093},
  MONTH = {02},
  PAGES = {1737--1746},
  PMID = {18282824},
  TITLE = {{Deep Learning with Limited Numerical Precision}},
  URL = {http://jmlr.org/proceedings/papers/v37/gupta15.pdf},
  YEAR = {2015},
}

@INPROCEEDINGS{vanhoucke2011improving,
  ABSTRACT = {Recent advances in deep learning have made the use of large, deep neural net- works with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 fixed-point instructions which provide a 3× improve- ment over an optimized floating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10× speedup over an unoptimized baseline and a 4× speedup over an aggressively optimized floating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware.},
  AUTHOR = {Vanhoucke, Vincent and Senior, Andrew and Mao, Mz},
  BOOKTITLE = {{Proc. Deep Learning and {\ldots}}},
  ISSN = {9781450329569},
  PAGES = {1--8},
  TITLE = {{Improving the speed of neural networks on CPUs}},
  URL = {http://research.google.com/pubs/archive/37631.pdf},
  VOLUME = {1},
  YEAR = {2011},
}

@INPROCEEDINGS{Denil2013predicting,
  ABSTRACT = {We demonstrate that there is signiﬁcant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95{\%} of the weights of a network without any drop in accuracy.},
  ARXIVID = {1306.0543},
  AUTHOR = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and de Freitas, Nando},
  BOOKTITLE = {{Neural Information Processing Systems (NIPS)}},
  EPRINT = {1306.0543},
  EPRINTTYPE = {arXiv},
  PAGES = {2148--2156},
  TITLE = {{Predicting Parameters in Deep Learning}},
  URL = {http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning},
  YEAR = {2013},
}

@INPROCEEDINGS{conf/cvpr/RigamontiSLF13,
  AUTHOR = {Rigamonti, Roberto and Sironi, Amos and Lepetit, Vincent and Fua, Pascal},
  BOOKTITLE = {{Computer Vision and Pattern Recognition (CVPR)}},
  KEYWORDS = {dblp},
  PAGES = {2754--2761},
  PUBLISHER = {IEEE},
  TITLE = {{Learning Separable Filters.}},
  YEAR = {2013},
}

@INPROCEEDINGS{journals/corr/JaderbergVZ14,
  AUTHOR = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  BOOKTITLE = {{British Machine Vision Conference}},
  KEYWORDS = {dblp},
  TITLE = {{Speeding up Convolutional Neural Networks with Low Rank Expansions.}},
  YEAR = {2014},
}

@INCOLLECTION{mamalet2012simplifying,
  AUTHOR = {Mamalet, Franck and Garcia, Christophe},
  BOOKTITLE = {{Artificial Neural Networks and Machine Learning--ICANN 2012}},
  PAGES = {58--65},
  PUBLISHER = {Springer},
  TITLE = {{Simplifying convnets for fast learning}},
  YEAR = {2012},
}

@MISC{Hochreiter01gradientflow,
  ABSTRACT = {Introduction Recurrent networks (crossreference Chapter 12) can, in principle, use their feedback connections to store representations of recent input events in the form of activations. The most widely used algorithms for learning what to put in short-term memory, however, take too much time to be feasible or do not work well at all, especially when minimal time lags between inputs and corresponding teacher signals are long. Although theoretically fascinating, they do not provide clear practical advantages over, say, backprop in feedforward networks with limited time windows (see crossreference Chapters 11 and 12). With conventional {\&}034;algorithms based on the computation of the complete gradient{\&}034;, such as {\&}034;Back-Propagation Through Time{\&}034; (BPTT, e.g., 22, 27, 26) or {\&}034;Real-Time Recurrent Learning{\&}034; (RTRL, e.g., 21) error signals {\&}034;flowing backwards in time{\&}034; tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error ex},
  ARXIVID = {arXiv:1011.1669v3},
  AUTHOR = {Hochreiter, Sepp and Bengio, Y and Frasconi, Paolo and Schmidhuber, J},
  BOOKTITLE = {{A Field Guide to Dynamical Recurrent Networks}},
  DOI = {10.1109/9780470544037.ch14},
  EPRINT = {arXiv:1011.1669v3},
  EPRINTTYPE = {arXiv},
  ISBN = {978-0-7803-5369-5},
  ISSN = {1098-6596},
  PAGES = {237--243},
  PMID = {25246403},
  TITLE = {{Gradient flow in recurrent nets: the difficulty of learning long-term dependencies}},
  URL = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.7321{&}rep=rep1{&}type=pdf},
  YEAR = {2001},
}

@ARTICLE{Jia2014,
  ABSTRACT = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
  ARXIVID = {1408.5093},
  AUTHOR = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  DOI = {10.1145/2647868.2654889},
  EPRINT = {1408.5093},
  EPRINTTYPE = {arXiv},
  ISBN = {9781450330633},
  ISSN = {10636919},
  JOURNALTITLE = {ACM International Conference on Multimedia},
  KEYWORDS = {computation; computer vision; corresponding authors; machine learning; neural networks; open source; parallel},
  PAGES = {675--678},
  PMID = {18267787},
  TITLE = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
  URL = {http://arxiv.org/abs/1408.5093},
  YEAR = {2014},
}

@INPROCEEDINGS{zhou2014learning,
  ABSTRACT = {Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.},
  AUTHOR = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
  BOOKTITLE = {{Advances in Neural Information Processing Systems 27}},
  ISSN = {10495258},
  PAGES = {487--495},
  TITLE = {{Learning Deep Features for Scene Recognition using Places Database}},
  URL = {http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf},
  YEAR = {2014},
}

@INPROCEEDINGS{Lin2014,
  AUTHOR = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  BOOKTITLE = {{International Conference on Learning Representations (ICLR)}},
  SERIES = {{2014}},
  TITLE = {{Network in network}},
  YEAR = {2014},
}

@REPORT{CIFAR10,
  ABSTRACT = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
  ARXIVID = {arXiv:1011.1669v3},
  AUTHOR = {Krizhevsky, Alex},
  BOOKTITLE = {{{\ldots} Science Department, University of Toronto, Tech. {\ldots}}},
  EPRINT = {arXiv:1011.1669v3},
  EPRINTTYPE = {arXiv},
  INSTITUTION = {Univ. Toronto},
  ISBN = {9788578110796},
  ISSN = {1098-6596},
  PAGES = {1--60},
  PMID = {25246403},
  TITLE = {{Learning Multiple Layers of Features from Tiny Images}},
  TYPE = {Technical Report},
  URL = {http://scholar.google.com/scholar?hl=en{&}btnG=Search{&}q=intitle:Learning+Multiple+Layers+of+Features+from+Tiny+Images{#}0},
  YEAR = {2009},
}

@INPROCEEDINGS{goodfellow2013maxout,
  ABSTRACT = {We consider the problem of designing mod-els to leverage a recently introduced ap-proximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a nat-ural companion to dropout) designed to both facilitate optimization by dropout and im-prove the accuracy of dropout's fast approxi-mate model averaging technique. We empir-ically verify that the model successfully ac-complishes both of these tasks. We use max-out and dropout to demonstrate state of the art classification performance on four bench-mark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
  ARXIVID = {1302.4389},
  AUTHOR = {Goodfellow, Ian J and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  BOOKTITLE = {{Proceedings of the 30th International Conference on Machine Learning (ICML)}},
  EPRINT = {1302.4389},
  EPRINTTYPE = {arXiv},
  PAGES = {1319--1327},
  TITLE = {{Maxout Networks}},
  VOLUME = {28},
  YEAR = {2013},
}

@INPROCEEDINGS{Denton2014efficient,
  ABSTRACT = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1{\%} of the original model.},
  ARXIVID = {1404.0736},
  AUTHOR = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  BOOKTITLE = {{arXiv}},
  EPRINT = {1404.0736},
  EPRINTTYPE = {arXiv},
  ISSN = {10495258},
  NUMBER = {1},
  PAGES = {1--11},
  TITLE = {{Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation}},
  URL = {http://arxiv.org/abs/1404.0736},
  YEAR = {2014},
}

@ARTICLE{journals/pami/SironiTRLF15,
  ABSTRACT = {Learning filters to produce sparse image representations in terms of $\backslash$nover complete dictionaries has emerged as a powerful way to create image $\backslash$nfeatures for many different purposes. Unfortunately, these filters are usually $\backslash$nboth numerous and non-separable, making their use computationally expensive. In $\backslash$nthis paper, we show that such filters can be computed as linear combinations of $\backslash$na smaller number of separable ones, thus greatly reducing the computational $\backslash$ncomplexity at no cost in terms of performance. This makes filter learning $\backslash$napproaches practical even for large images or 3D volumes, and we show that we $\backslash$nsignificantly outperform state-of-the-art methods on the linear structure $\backslash$nextraction task, in terms of both accuracy and speed. Moreover, our approach is $\backslash$ngeneral and can be used on generic filter banks to reduce the complexity of the $\backslash$nconvolutions.},
  AUTHOR = {Sironi, Amos and Tekin, Bugra and Rigamonti, Roberto and Lepetit, Vincent and Fua, Pascal},
  DOI = {10.1109/TPAMI.2014.2343229},
  ISBN = {978-0-7695-4989-7},
  ISSN = {01628828},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  KEYWORDS = {Convolutional neural networks; Convolutional sparse coding; Features extraction; Filter learning; Image denoising; Segmentation of linear structures; Separable convolution; Tensor decomposition},
  NUMBER = {1},
  PAGES = {94--106},
  PMID = {26353211},
  TITLE = {{Learning separable filters}},
  VOLUME = {37},
  YEAR = {2015},
}

@ARTICLE{journals/corr/LebedevGROL14,
  ABSTRACT = {We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discrim-inative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it yields larger CPU speedups at the cost of lower accuracy drops compared to previous approaches. For the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1{\%} from 91{\%} to 90{\%}). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of 1{\%} increase of the overall top-5 classification error.},
  ARXIVID = {arXiv:1412.6553v2},
  AUTHOR = {Lebedev, Vadim and Ganin, Yaroslav and Rakhuba1, Maksim and Oseledets, Ivan and Lempitsky, Victor},
  EPRINT = {arXiv:1412.6553v2},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {International Conference on Learning Representations (ICLR)},
  KEYWORDS = {dblp},
  PAGES = {1--10},
  TITLE = {{Speeding-Up Convolutional Neural Networks Using Fine-tuned CP-Decomposition}},
  VOLUME = {abs/1412.6},
  YEAR = {2015},
}

@ARTICLE{mathieu2013fast,
  ABSTRACT = {Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a signiﬁcant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.},
  ARXIVID = {arXiv:1312.5851v5},
  AUTHOR = {Mathieu, Michael and Henaff, Mikael and LeCun, Y},
  EPRINT = {arXiv:1312.5851v5},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {International Conference on Learning Representations (ICLR)},
  PAGES = {1--9},
  TITLE = {{Fast Training of Convolutional Networks through FFTs}},
  URL = {http://arxiv.org/abs/1312.5851},
  YEAR = {2014},
}

@ARTICLE{rippel2015spectral,
  ABSTRACT = {Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its ad- vantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality re- duction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strate- gies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by random- ized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral param- eterization of convolutional filters. While this leaves the underlying model un- changed, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training. 1},
  ARXIVID = {1506.03767},
  AUTHOR = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P},
  EPRINT = {1506.03767},
  EPRINTTYPE = {arXiv},
  ISSN = {10495258},
  JOURNALTITLE = {Advances in Neural Information Processing Systems 28},
  PAGES = {2440--2448},
  TITLE = {{Spectral Representations for Convolutional Neural Networks}},
  URL = {http://papers.nips.cc/paper/5649-spectral-representations-for-convolutional-neural-networks.pdf},
  YEAR = {2015},
}

@INPROCEEDINGS{Chen2015,
  ABSTRACT = {As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.},
  ARXIVID = {1504.04788},
  AUTHOR = {Chen, Wenlin and Wilson, James T. and Tyree, Stephen and Weinberger, Kilian Q. and Chen, Yixin},
  BOOKTITLE = {{Proceedings of The 32nd International Conference on Machine Learning}},
  EDITOR = {Bach, Francis R and Blei, David M},
  EPRINT = {1504.04788},
  EPRINTTYPE = {arXiv},
  ISBN = {9781510810587},
  KEYWORDS = {dblp},
  PAGES = {2285--2294},
  PUBLISHER = {JMLR.org},
  SERIES = {{JMLR Proceedings}},
  TITLE = {{Compressing Neural Networks with the Hashing Trick}},
  URL = {http://arxiv.org/abs/1504.04788},
  VOLUME = {37},
  YEAR = {2015},
}

@INPROCEEDINGS{Cogswell2016,
  AUTHOR = {Cogswell, Michael and Ahmed, Faruk and Girshick, Ross B and Zitnick, Larry and Batra, Dhruv},
  BOOKTITLE = {{International Conference on Learning Representations}},
  TITLE = {{Reducing Overfitting in Deep Networks by Decorrelating Representations.}},
  YEAR = {2016},
}

@ARTICLE{Jhurani2015,
  AUTHOR = {Jhurani, Chetan and Mullowney, Paul},
  JOURNALTITLE = {Journal of Parallel and Distributed Computing},
  PAGES = {133--140},
  PUBLISHER = {Elsevier},
  TITLE = {{A GEMM interface and implementation on NVIDIA GPUs for multiple small matrices}},
  VOLUME = {75},
  YEAR = {2015},
}

@INPROCEEDINGS{yi2016lift,
  AUTHOR = {Yi, Kwang Moo and Trulls, Eduard and Lepetit, Vincent and Fua, Pascal},
  BOOKTITLE = {{European Conference on Computer Vision (ECCV)}},
  LOCATION = {Amsterdarm},
  TITLE = {{LIFT: Learned Invariant Feature Transform}},
  YEAR = {2016},
}

@MISC{conf/cvpr/ShottonFCSFMKB11,
  ABSTRACT = {We propose a new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing, etc. Finally we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes. The system runs at 200 frames per second on consumer hardware. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect of several training parameters. We achieve state of the art accuracy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching.},
  AUTHOR = {Shotton, Jamie and Fitzgibbon, Andrew and Cook, Mat and Sharp, Toby and Finocchio, Mark and Moore, Richard and Kipman, Alex and Blake, Andrew},
  BOOKTITLE = {{Computer Vision and Pattern Recognition (CVPR)}},
  INSTITUTION = {Microsoft Research Cambridge {\&} Xbox Incubation},
  ISBN = {9781457703935},
  ISSN = {10636919},
  NUMBER = {3},
  PAGES = {1297--1304},
  PUBLISHER = {IEEE},
  TITLE = {{Real-time human pose recognition in parts from single depth images}},
  URL = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995316},
  VOLUME = {2},
  YEAR = {2011},
}

@REPORT{Sethi1990,
  AUTHOR = {Sethi, I K},
  BOOKTITLE = {{Proceedings of the {\{}IEEE{\}}}},
  INSTITUTION = {Dept. of Computer Sci, Wayne State Univ.},
  NUMBER = {10},
  PAGES = {1605--1613},
  TITLE = {{Entropy Nets: From Decison Trees to Neural Networks}},
  TYPE = {techreport},
  VOLUME = {78},
  YEAR = {1990},
}

@INPROCEEDINGS{montillo2011entangled,
  ABSTRACT = {This work addresses the challenging problem of simultaneously segmenting multiple anatomical structures in highly varied CT scans. We propose the entangled decision forest (EDF) as a new discriminative classifier which augments the state of the art decision forest, resulting in higher prediction accuracy and shortened decision time. Our main contribution is two-fold. First, we propose entangling the binary tests applied at each tree node in the forest, such that the test result can depend on the result of tests applied earlier in the same tree and at image points offset from the voxel to be classified. This is demonstrated to improve accuracy and capture long-range semantic context. Second, during training, we propose injecting randomness in a guided way, in which node feature types and parameters are randomly drawn from a learned (nonuniform) distribution. This further improves classification accuracy. We assess our probabilistic anatomy segmentation technique using a labeled database of CT image volumes of 250 different patients from various scan protocols and scanner vendors. In each volume, 12 anatomical structures have been manually segmented. The database comprises highly varied body shapes and sizes, a wide array of pathologies, scan resolutions, and diverse contrast agents. Quantitative comparisons with state of the art algorithms demonstrate both superior test accuracy and computational efficiency.},
  AUTHOR = {Montillo, Albert and Shotton, Jamie and Winn, John and Iglesias, Juan Eugenio and Metaxas, Dimitri and Criminisi, Antonio},
  BOOKTITLE = {{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}},
  DOI = {10.1007/978-3-642-22092-0_16},
  ISBN = {9783642220913},
  ISSN = {03029743},
  KEYWORDS = {CT; Entanglement; auto-context; decision forests; segmentation},
  PAGES = {184--196},
  PMID = {21761656},
  TITLE = {{Entangled decision forests and their application for semantic segmentation of CT images}},
  VOLUME = {6801 LNCS},
  YEAR = {2011},
}

@INPROCEEDINGS{BuloKontsch2014,
  AUTHOR = {{Rota Bulò}, S and Kontschieder, P},
  BOOKTITLE = {{Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition}},
  MONTH = {06},
  TITLE = {{Neural Decision Forests for Semantic Image Labelling}},
  YEAR = {2014},
}

@INPROCEEDINGS{Snoek2012,
  ARXIVID = {1206.2944},
  AUTHOR = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan Prescott},
  BOOKTITLE = {{Advances in Neural Information Processing Systems}},
  EPRINT = {1206.2944},
  EPRINTTYPE = {arXiv},
  ISBN = {9781627480031},
  ISSN = {10495258},
  PAGES = {2960--2968},
  PMID = {9377276},
  TITLE = {{Practical Bayesian Optimization of Machine Learning Algorithms}},
  YEAR = {2012},
}

@ARTICLE{zhang2017primal,
  AUTHOR = {Zhang, Ting and Qi, Guo-Jun and Xiao, Bin and Wang, Jingdong},
  JOURNALTITLE = {arXiv preprint arXiv:1707.02725},
  OWNER = {yani},
  TIMESTAMP = {2017.09.13},
  TITLE = {{Primal-Dual Group Convolutions for Deep Neural Networks}},
  YEAR = {2017},
}

@ARTICLE{changpinyo2017power,
  AUTHOR = {Changpinyo, Soravit and Sandler, Mark and Zhmoginov, Andrey},
  JOURNALTITLE = {arXiv preprint arXiv:1702.06257},
  TITLE = {The power of sparsity in convolutional neural networks},
  YEAR = {2017},
}

