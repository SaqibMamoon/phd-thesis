\documentclass[thesis]{subfiles}

\begin{document}
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

\begin{chapquote}{Marvin Minsky, \textit{Prologue: A View from 1988, Perceptrons}}
``The marvelous powers of the brain emerge not from any single, uniformly structured connectionist network but from highly evolved arrangements of smaller, specialized networks which are interconnected in very specific ways.''
\end{chapquote}

%********************************** %First Section  **************************************
Deep learning has in recent years come to dominate the previously separate fields of research in machine learning, computer vision, natural language understanding and speech recognition. The this would have been unbelievable less than 5 years ago,  belies the power of the method. Deep learning is of course only the latest in a long history of connectionist learning research, and while the breakthroughs in training deep networks are real, many relatively simple questions of learning in neural networks remain unanswered, while the research community has continued to discover increasingly more applications for deep learning.

This lack of understanding in both the optimization and structure of deep networks has meant that contemporary deep network architectures for image classification have high computational and memory complexity. This is a direct result of the inability to identify the optimal architecture for datasets. Instead, the approach advocated by many researchers in the field has been to train large monolithic networks with a larger than required capacity, and strong regularization -- an approach that has found success in practice.

Instead, in this work, we propose that carefully designing networks in consideration of our prior knowledge of the task can improve the memory and compute efficiency of state-of-the art networks, and even increase accuracy through structurally induced regularization. 


The rest of this thesis is organised as follows:
\begin{description}
	\item[Chapter \ref{background}] explores the background of decision forests, neural networks and their applications, with particular emphasis on image classification.
	\item[Chapter \ref{conditionalnetworks}] presents work towards conditional computation in deep neural networks, conditional networks, allowing faster inference. 
	\item[Chapter~\ref{lowrankfilters}] addresses the spatial extents of convolutional filters, showings how to exploit our knowledge of the low-rank nature of most filters learned in image recognition, and learn a low-rank basis for filters~\ref{lowrankfilters}.
	\item[Chapter \ref{deeproots}] describes a new method for creating computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. This allows a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy.
	
\end{description}

\end{document}
