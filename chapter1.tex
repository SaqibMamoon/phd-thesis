\documentclass[thesis]{subfiles}

\begin{document}
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

%********************************** %First Section  **************************************
\section{On Methods of Discriminative Classification} %Section - 1.1

Two methods of discriminative classification, \emph{Neural Networks} and \emph{Decision Forests}, have recently dominated the field of Computer Vision. Deep neural networks have even replaced the research in local features (e.g. SIFT), providing end-to-end learning from pixels to objective. Much work has been done on improving both methods and exploring their applications - with academic and even commercial success. For example, decision forests are used to find the body pose of a person with the Kinect~\cite{conf/cvpr/ShottonFCSFMKB11} used in game consoles. Deep convolutional neural networks (\emph{CNNs}) on the other hand have recently surpassed human accuracy on what was considered one of the most challenging outstanding problems in computer vision, object class recognition~\cite{journals/corr/HeZR015}, and are already finding their ways into applications such as Google Photos~\cite{googlephotos}. However, the important fact that these two methods are related often seems to be all but forgotten. In the early 1990s Sethi et al.~\cite{Sethi1990} showed that any decision tree can be represented as a neural network with one hidden layer, however the converse does not necessarily hold true.

Despite this fundamental relationship, decision forests and neural networks have such distinct and mutually exclusive strengths and weaknesses that it is not surprising that they are themselves usually considered to be distinct. Decision forests require vast amounts of labelled data, proportional to the number of classes and tree depth, since samples are ``diluted'' down the tree, while, with appropriate regularization, neural networks can be trained with far more parameters than actual samples. At test time neural networks are opaque giving little understanding, while decision forests are more intuitive - each node having an explicit decision on the input data and even describe per-class statistics. The routing of decision forests makes it easy to distribute computation, while the high connectivity of neural networks makes model parallelism difficult and inefficient. Decision forests are extremely fast at test time, due to sample routing only a small part of a tree need be computed, i.e. conditional computation, neural networks must on the other hand compute the response at every node even if many of these responses are approximately zero.

The ideal discriminative model desired for most tasks would have all the advantages of both neural networks and decision forests, and none of the weaknesses. It would have good generalization with computational efficiency, lend itself to semantic understanding and yet have sufficient functional complexity to solve complex problems. We intend to explore the continuum of discriminative models that exist between decision forests and neural networks, to try and find such a balance. We will explore the theory and applications behind such models, with a focus on contemporary problems such as object class recognition, \ie the ImageNet ILSVRC challenge which has been the focus of much of the recent work on deep learning.

The rest of this proposal is organised as follows: Chapter \ref{background} explores the background of decision forests, neural networks and their applications, with particular emphasis on image classification. Chapter \ref{methodology} details the methods used to train convolutional neural networks. Chapter \ref{firstyear} presents the key contribution of our first year's work, conditional networks. Chapter \ref{nexttwoyears} proposes a research plan based on further exploring methods for conditional networks, and applications thereof.

%The prototypical neural network is trained with back-propogration, a gradient based form of global optimization. Tree training is typically a greedy layer-wise construction.
%consists of several levels of nodes, each of which is fully-connected to all the nodes of a previous layer. These neurons perform

\end{document}
