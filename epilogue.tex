% !TEX root = thesis.tex
\documentclass[thesis]{subfiles}

\begin{document}
%*******************************************************************************
%*********************************** Epilogue *****************************
%*******************************************************************************

\chapter{Research Epilogue}
\label{epilogue}
%********************************** %First Section  **************************************
Research in deep learning has taken on a new rapidity since the adoption of pre-prints, and the explosion of interest in field. Rather than being bound to the annual conference schedule, new research is released on a weekly basis. Presenting a paper at a contemporary conference, one is now in the odd situation of having to relate the `new' research being presented to the 6--12 months of followup research in the field. Compare this to even a few years ago, when publication of new research was withheld until a conference paper acceptance, perhaps a couple of months before the conference.

This thesis represents the ultimate presentation of the research we have undertaken, and just as in a conference, it must also be presented in the context and timeline of the followup research and applications it has already inspired.

In this section, we will briefly outline the significant derivative papers published after the original pre-print publication of the research we have presented here, along with their pre-print dates. For reference, the initial public release of the papers behind the work presented in this thesis are outlined below:

\begin{itemize}
    \item Training CNNs with Low-Rank Filters for Efficient Image Classification~\citep{Ioannou2016}
    \begin{description}
        \item[Pre-print] arXiv:1511.06744 (30 Nov.\ 2015)
        \item[Peer-reviewed publication date] May 2, 2016
    \end{description}
    \item Decision Forests, Convolutional Networks and the Models in-Between~\citep{Ioannou2015}
    \begin{description}
        \item[Pre-print] 3 Mar.\ 2016
        %\item[MSR internal technical report date] April, 2015
    \end{description}
    \item Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups~\citep{ioannou2016e}
    \begin{description}
        \item[Pre-print] arXiv:1605.06489 (20 May 2016)
        \item[Peer-reviewed publication date] 21 July, 2017
    \end{description}
\end{itemize}

\section{Rethinking the Inception Architecture for Computer Vision}
\begin{description}
    \item[Pre-print] arxiv:1512.00567 (2 Dec.\ 2015)
\end{description}
The Google Brain research team published an update to the Inception architecture around 3 weeks after our publication, making it likely that their work was independent. Nevertheless, the method they present is identical to our proposal~\citep{Ioannou2016} --- the training of the Inception architecture with low-rank (\ie{}1$\times$3 and 3$\times$1) filters to reduce computation and improve generalization. They call these `factorized filters' which we disagree with, since being simply concatenated they are not a factorization (\ie separation of a multiplication), rather we argue they are linearly combined, and so represent a basis.

Nevertheless, the method proposed in this paper (and identical to our proposed method) forms the basis of all current Inception architectures, and are now used in all Google deep learning backed products, for example \copyright{}Google Photos.

\section{Xception: Deep Learning with Depthwise Separable Convolutions}
\begin{description}
    \item[Pre-print] arXiv:1610.02357 (7 Oct.\ 2016)
\end{description}
\mynote{TODO}

\section{Aggregated Residual Transformations for Deep Neural Networks}
\begin{description}
    \item[Pre-print] arXiv:1611.05431 (16 Nov.\ 2016)
\end{description}
The ResNeXt architecture, as proposed in this paper, cites our work~\citep{ioannou2016e}, and while the method is identical to ours, they explore a different compute-generalization trade-off. Rather than using the better representation to save parameters and computation as in~\citep{ioannou2016e}, the authors propose to maintain the original computational footprint of the model, and increase the number of filters learned.
\mynote{TODO}

\section{Interleaved Group Convolutions for Deep Neural Networks}
\begin{description}
    \item[Pre-print] arXiv:1707.02725 (7 Oct.\ 2016)
    \item[Peer-reviewed publication date] October 22, 2017
\end{description}
\citet{zhang2017primal}
\mynote{TODO}
\end{document}
